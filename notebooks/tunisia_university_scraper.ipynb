{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8c9629b",
   "metadata": {},
   "source": [
    "# üáπüá≥ Tunisian University Guide Comprehensive Scraper\n",
    "\n",
    "This notebook implements a complete scraping solution for the Tunisian University Orientation website (https://guide-orientation.rnu.tn/).\n",
    "\n",
    "## üéØ Objectives\n",
    "- Extract all combinations of:\n",
    "  - **ŸÜŸàÿπ ÿßŸÑÿ®ÿßŸÉÿßŸÑŸàÿ±Ÿäÿß** (Baccalaureate types)\n",
    "  - **ÿßŸÑÿ¨ÿßŸÖÿπÿ©** (Universities) \n",
    "  - **ÿßŸÑŸÖÿ§ÿ≥ÿ≥ÿ©** (Institutions)\n",
    "  - **ÿßŸÑÿ¥ÿπÿ®ÿ©/ÿßŸÑÿ•ÿ¨ÿßÿ≤ÿ©** (Specializations/Degrees)\n",
    "\n",
    "- Scrape detailed information from ramz popup pages including:\n",
    "  - University and institution details\n",
    "  - Program specifications\n",
    "  - Historical admission scores\n",
    "  - Entry requirements\n",
    "\n",
    "## üìä Expected Output\n",
    "A comprehensive CSV dataset with ~3,161 specialization records containing all program details and statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91782cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import requests\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from urllib.parse import urljoin, parse_qs, urlparse\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Dict, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27ef755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration and Data Structures\n",
    "BASE_URL = \"https://guide-orientation.rnu.tn\"\n",
    "MAX_WORKERS = 10  # Parallel processing workers\n",
    "DELAY = 1.0       # Delay between requests (seconds)\n",
    "\n",
    "# Ensure data directory exists\n",
    "os.makedirs('../data', exist_ok=True)\n",
    "\n",
    "@dataclass\n",
    "class SpecializationDetail:\n",
    "    \"\"\"Data class for specialization details\"\"\"\n",
    "    ramz_code: str\n",
    "    ramz_url: str\n",
    "    bac_type: str = \"\"\n",
    "    university: str = \"\"\n",
    "    governorate: str = \"\"\n",
    "    institution: str = \"\"\n",
    "    address: str = \"\"\n",
    "    phone: str = \"\"\n",
    "    specialization: str = \"\"\n",
    "    training_field: str = \"\"\n",
    "    specializations: str = \"\"\n",
    "    measure: str = \"\"\n",
    "    capacity_2025: str = \"\"\n",
    "    requires_test: str = \"\"\n",
    "    geographic_distribution: str = \"\"\n",
    "    conditions: str = \"\"\n",
    "    study_duration: str = \"\"\n",
    "    last_oriented_score_2024: str = \"\"\n",
    "    score_history: dict = None\n",
    "    extraction_timestamp: str = \"\"\n",
    "\n",
    "# Session configuration\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "    'Accept-Language': 'ar,en;q=0.5',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Referer': BASE_URL\n",
    "})\n",
    "\n",
    "print(\"‚úÖ Configuration and data structures defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e8d77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Scraping Functions\n",
    "\n",
    "def get_bac_types():\n",
    "    \"\"\"Extract baccalaureate types from main page\"\"\"\n",
    "    print(\"üîç Extracting baccalaureate types...\")\n",
    "    \n",
    "    try:\n",
    "        response = session.get(f\"{BASE_URL}/index.php\")\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Look for select elements with bac types\n",
    "        bac_options = []\n",
    "        selects = soup.find_all('select')\n",
    "        \n",
    "        for select in selects:\n",
    "            options = select.find_all('option')\n",
    "            for option in options:\n",
    "                value = option.get('value')\n",
    "                text = option.get_text().strip()\n",
    "                if value and text and value != '0' and 'ÿ•ÿÆÿ™ÿ±' not in text:\n",
    "                    bac_options.append({'value': value, 'text': text})\n",
    "        \n",
    "        # Remove duplicates\n",
    "        unique_options = []\n",
    "        seen_values = set()\n",
    "        for option in bac_options:\n",
    "            if option['value'] not in seen_values:\n",
    "                unique_options.append(option)\n",
    "                seen_values.add(option['value'])\n",
    "        \n",
    "        print(f\"‚úÖ Found {len(unique_options)} baccalaureate types\")\n",
    "        return unique_options\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error extracting bac types: {e}\")\n",
    "        return []\n",
    "\n",
    "def get_ramz_links_for_bac(bac_value, bac_text):\n",
    "    \"\"\"Get all ramz links for a specific baccalaureate type\"\"\"\n",
    "    print(f\"üîó Getting ramz links for {bac_text}...\")\n",
    "    \n",
    "    try:\n",
    "        # First get the main page to understand the form structure\n",
    "        main_response = session.get(f\"{BASE_URL}/index.php\")\n",
    "        main_response.raise_for_status()\n",
    "        main_soup = BeautifulSoup(main_response.content, 'html.parser')\n",
    "        \n",
    "        # Look for the actual form action\n",
    "        form = main_soup.find('form')\n",
    "        form_action = form.get('action') if form else None\n",
    "        \n",
    "        # Try different possible search URLs\n",
    "        possible_urls = [\n",
    "            f\"{BASE_URL}/ar/dynamique/index_ar.php\",\n",
    "            f\"{BASE_URL}/dynamique/index_ar.php\", \n",
    "            f\"{BASE_URL}/ar/index_ar.php\",\n",
    "            f\"{BASE_URL}/index_ar.php\"\n",
    "        ]\n",
    "        \n",
    "        if form_action:\n",
    "            full_action = urljoin(BASE_URL, form_action)\n",
    "            possible_urls.insert(0, full_action)\n",
    "        \n",
    "        # Try each URL until we find one that works\n",
    "        for search_url in possible_urls:\n",
    "            try:\n",
    "                print(f\"   Trying URL: {search_url}\")\n",
    "                \n",
    "                # Try GET request first (some forms use GET)\n",
    "                get_params = {\n",
    "                    'nbac': bac_value,\n",
    "                    'univ': '',\n",
    "                    'inst': '',\n",
    "                    'spec': ''\n",
    "                }\n",
    "                \n",
    "                response = session.get(search_url, params=get_params)\n",
    "                \n",
    "                # If GET doesn't work, try POST\n",
    "                if response.status_code != 200:\n",
    "                    form_data = {\n",
    "                        'nbac': bac_value,\n",
    "                        'univ': '',\n",
    "                        'inst': '', \n",
    "                        'spec': ''\n",
    "                    }\n",
    "                    response = session.post(search_url, data=form_data)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                    \n",
    "                    # Save search results for debugging\n",
    "                    with open(f'../data/search_debug_bac_{bac_value}.html', 'w', encoding='utf-8') as f:\n",
    "                        f.write(soup.prettify())\n",
    "                    \n",
    "                    # Extract ramz links\n",
    "                    ramz_links = extract_ramz_links_from_soup(soup, bac_value, bac_text)\n",
    "                    \n",
    "                    if ramz_links:\n",
    "                        print(f\"‚úÖ Found {len(ramz_links)} ramz links for {bac_text}\")\n",
    "                        return ramz_links\n",
    "                    else:\n",
    "                        print(f\"   No ramz links found in response from {search_url}\")\n",
    "                        \n",
    "                else:\n",
    "                    print(f\"   HTTP {response.status_code} from {search_url}\")\n",
    "                    \n",
    "            except Exception as url_error:\n",
    "                print(f\"   Error with {search_url}: {url_error}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"‚ùå All URLs failed for {bac_text}\")\n",
    "        return []\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error getting ramz links for {bac_text}: {e}\")\n",
    "        return []\n",
    "\n",
    "def extract_ramz_links_from_soup(soup, bac_value, bac_text):\n",
    "    \"\"\"Extract ramz links from search results HTML\"\"\"\n",
    "    ramz_links = []\n",
    "    \n",
    "    # Method 1: Find all links with PopupCentrer\n",
    "    links = soup.find_all('a', href=re.compile(r'javascript:PopupCentrer'))\n",
    "    \n",
    "    for link in links:\n",
    "        href = link.get('href')\n",
    "        if 'filiere.php' in href:\n",
    "            # Extract URL from JavaScript function\n",
    "            url_match = re.search(r'\"([^\"]*filiere\\.php[^\"]*)\"', href)\n",
    "            if url_match:\n",
    "                relative_url = url_match.group(1)\n",
    "                full_url = urljoin(BASE_URL, relative_url)\n",
    "                \n",
    "                # Extract ramz code from URL\n",
    "                parsed_url = urlparse(relative_url)\n",
    "                query_params = parse_qs(parsed_url.query)\n",
    "                ramz_id = query_params.get('id', [None])[0]\n",
    "                \n",
    "                if ramz_id:\n",
    "                    # Extract ramz code (remove prefix if present)\n",
    "                    ramz_code = re.sub(r'^\\d+', '', ramz_id) if len(ramz_id) > 5 else ramz_id\n",
    "                    \n",
    "                    ramz_links.append({\n",
    "                        'ramz_code': ramz_code,\n",
    "                        'ramz_id': ramz_id,\n",
    "                        'url': full_url,\n",
    "                        'bac_value': bac_value,\n",
    "                        'bac_text': bac_text\n",
    "                    })\n",
    "    \n",
    "    # Method 2: Look in table cells for ramz codes\n",
    "    tables = soup.find_all('table')\n",
    "    for table in tables:\n",
    "        rows = table.find_all('tr')\n",
    "        for row in rows:\n",
    "            cells = row.find_all('td')\n",
    "            for cell in cells:\n",
    "                # Look for ramz code patterns in cell text\n",
    "                cell_text = cell.get_text().strip()\n",
    "                ramz_match = re.search(r'\\b\\d{5,6}\\b', cell_text)\n",
    "                \n",
    "                if ramz_match:\n",
    "                    ramz_code = ramz_match.group()\n",
    "                    \n",
    "                    # Look for associated link\n",
    "                    cell_links = cell.find_all('a', href=re.compile(r'PopupCentrer'))\n",
    "                    for cell_link in cell_links:\n",
    "                        href = cell_link.get('href')\n",
    "                        url_match = re.search(r'\"([^\"]*filiere\\.php[^\"]*)\"', href)\n",
    "                        if url_match:\n",
    "                            relative_url = url_match.group(1)\n",
    "                            full_url = urljoin(BASE_URL, relative_url)\n",
    "                            \n",
    "                            ramz_links.append({\n",
    "                                'ramz_code': ramz_code,\n",
    "                                'ramz_id': ramz_code,\n",
    "                                'url': full_url,\n",
    "                                'bac_value': bac_value,\n",
    "                                'bac_text': bac_text\n",
    "                            })\n",
    "    \n",
    "    # Method 3: Direct link extraction from href attributes\n",
    "    all_links = soup.find_all('a', href=True)\n",
    "    for link in all_links:\n",
    "        href = link.get('href')\n",
    "        if 'filiere.php' in href and 'id=' in href:\n",
    "            # Direct link to filiere.php\n",
    "            if href.startswith('http'):\n",
    "                full_url = href\n",
    "            else:\n",
    "                full_url = urljoin(BASE_URL, href)\n",
    "            \n",
    "            # Extract ramz code\n",
    "            parsed_url = urlparse(href)\n",
    "            query_params = parse_qs(parsed_url.query)\n",
    "            ramz_id = query_params.get('id', [None])[0]\n",
    "            \n",
    "            if ramz_id:\n",
    "                ramz_code = re.sub(r'^\\d+', '', ramz_id) if len(ramz_id) > 5 else ramz_id\n",
    "                \n",
    "                ramz_links.append({\n",
    "                    'ramz_code': ramz_code,\n",
    "                    'ramz_id': ramz_id,\n",
    "                    'url': full_url,\n",
    "                    'bac_value': bac_value,\n",
    "                    'bac_text': bac_text\n",
    "                })\n",
    "    \n",
    "    # Remove duplicates based on ramz_code\n",
    "    unique_links = {}\n",
    "    for link in ramz_links:\n",
    "        key = link['ramz_code']\n",
    "        if key not in unique_links:\n",
    "            unique_links[key] = link\n",
    "    \n",
    "    return list(unique_links.values())\n",
    "\n",
    "print(\"‚úÖ Improved scraping functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b843e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML Parsing Functions\n",
    "\n",
    "def parse_ramz_detail_page(ramz_info, html_content):\n",
    "    \"\"\"Parse a ramz detail page HTML content\"\"\"\n",
    "    ramz_code = ramz_info['ramz_code']\n",
    "    url = ramz_info['url']\n",
    "    bac_text = ramz_info['bac_text']\n",
    "    \n",
    "    try:\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Initialize detail object\n",
    "        detail = SpecializationDetail(\n",
    "            ramz_code=ramz_code,\n",
    "            ramz_url=url,\n",
    "            bac_type=bac_text,\n",
    "            score_history={},\n",
    "            extraction_timestamp=datetime.now().isoformat()\n",
    "        )\n",
    "        \n",
    "        # Parse main data table\n",
    "        table = soup.find('table', class_='table')\n",
    "        if table:\n",
    "            rows = table.find_all('tr')\n",
    "            for row in rows:\n",
    "                cells = row.find_all('td')\n",
    "                if len(cells) >= 2:\n",
    "                    label = cells[0].get_text().strip()\n",
    "                    value_cell = cells[1]\n",
    "                    value = value_cell.get_text().strip()\n",
    "                    \n",
    "                    # Extract data based on Arabic labels\n",
    "                    if 'ÿßŸÑÿ¨ÿßŸÖÿπÿ©' in label and value:\n",
    "                        detail.university = value\n",
    "                    elif 'ÿßŸÑŸàŸÑÿßŸäÿ©' in label and value:\n",
    "                        detail.governorate = value\n",
    "                    elif 'ÿßŸÑŸÖÿ§ÿ≥ÿ≥ÿ©' in label:\n",
    "                        # Institution info might span multiple lines\n",
    "                        institution_text = value_cell.get_text()\n",
    "                        lines = [line.strip() for line in institution_text.split('\\\\n') if line.strip()]\n",
    "                        \n",
    "                        if lines:\n",
    "                            detail.institution = lines[0]\n",
    "                            \n",
    "                            # Look for address and phone in remaining text\n",
    "                            full_text = ' '.join(lines)\n",
    "                            \n",
    "                            address_match = re.search(r'ÿßŸÑÿπŸÜŸàÿßŸÜ\\\\s*:\\\\s*([^\\\\n\\\\r]+)', full_text)\n",
    "                            if address_match:\n",
    "                                detail.address = address_match.group(1).strip()\n",
    "                            \n",
    "                            phone_match = re.search(r'ÿßŸÑŸáÿßÿ™ŸÅ\\\\s*:\\\\s*([^\\\\n\\\\r]+)', full_text)\n",
    "                            if phone_match:\n",
    "                                detail.phone = phone_match.group(1).strip()\n",
    "                    \n",
    "                    elif 'ŸÖÿ¨ÿßŸÑ ÿßŸÑÿ™ŸÉŸàŸäŸÜ' in label and value:\n",
    "                        detail.training_field = value\n",
    "                    elif 'ÿßŸÑÿ¥ÿπÿ®ÿ© / ÿßŸÑÿ•ÿ¨ÿßÿ≤ÿ©' in label and value:\n",
    "                        detail.specialization = value\n",
    "                    elif 'ÿßŸÑÿ™ÿÆÿµÿµÿßÿ™' in label and value:\n",
    "                        detail.specializations = value\n",
    "                    elif 'ÿßŸÑŸÖŸÇŸäÿßÿ≥' in label and value:\n",
    "                        detail.measure = value\n",
    "                    elif 'ÿ∑ÿßŸÇÿ© ÿßŸÑÿ•ÿ≥ÿ™ÿπÿßÿ®' in label and value:\n",
    "                        detail.capacity_2025 = value\n",
    "                    elif 'ÿ¥ÿπÿ®ÿ© ÿ™ÿ™ÿ∑ŸÑÿ® ÿ•ÿÆÿ™ÿ®ÿßÿ±' in label and value:\n",
    "                        detail.requires_test = value\n",
    "                    elif 'ÿßŸÑÿ™ŸÜŸÅŸäŸÑ ÿßŸÑÿ¨ÿ∫ÿ±ÿßŸÅŸä' in label and value:\n",
    "                        detail.geographic_distribution = value\n",
    "                    elif 'ÿßŸÑÿ¥ÿ±Ÿàÿ∑' in label and value:\n",
    "                        detail.conditions = value\n",
    "                    elif 'ŸÖÿØÿ© ÿßŸÑÿØÿ±ÿßÿ≥ÿ©' in label and value:\n",
    "                        detail.study_duration = value\n",
    "                    elif 'ŸÖÿ¨ŸÖŸàÿπ ÿ¢ÿÆÿ± ŸÖŸàÿ¨Ÿá 2024' in label and value:\n",
    "                        detail.last_oriented_score_2024 = value\n",
    "        \n",
    "        # Extract score history from JavaScript data\n",
    "        scripts = soup.find_all('script')\n",
    "        for script in scripts:\n",
    "            if script.string:\n",
    "                script_content = script.string\n",
    "                \n",
    "                # Look for score data patterns\n",
    "                year_scores = re.findall(r'(20\\\\d{2})[^\\\\d]*(\\\\d+(?:\\\\.\\\\d+)?)', script_content)\n",
    "                for year, score in year_scores:\n",
    "                    if len(score) > 2:  # Likely a score\n",
    "                        detail.score_history[year] = score\n",
    "        \n",
    "        return detail\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error parsing ramz {ramz_code}: {e}\")\n",
    "        return None\n",
    "\n",
    "async def fetch_html_content(session, ramz_info):\n",
    "    \"\"\"Async function to fetch HTML content for a ramz page\"\"\"\n",
    "    try:\n",
    "        async with session.get(ramz_info['url']) as response:\n",
    "            if response.status == 200:\n",
    "                html = await response.text()\n",
    "                return ramz_info, html\n",
    "            else:\n",
    "                return ramz_info, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {ramz_info['ramz_code']}: {e}\")\n",
    "        return ramz_info, None\n",
    "\n",
    "print(\"‚úÖ HTML parsing functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05baa6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Analyze Website Structure\n",
    "\n",
    "def analyze_website_structure():\n",
    "    \"\"\"Analyze the main website to understand its structure\"\"\"\n",
    "    print(\"üîç Analyzing website structure...\")\n",
    "    \n",
    "    try:\n",
    "        # Get main page\n",
    "        response = session.get(f\"{BASE_URL}/index.php\")\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Save main page HTML\n",
    "        with open('../data/main_page_debug.html', 'w', encoding='utf-8') as f:\n",
    "            f.write(soup.prettify())\n",
    "        \n",
    "        print(\"‚úÖ Main page saved to ../data/main_page_debug.html\")\n",
    "        \n",
    "        # Analyze forms\n",
    "        forms = soup.find_all('form')\n",
    "        print(f\"\\\\nüìã Found {len(forms)} form(s):\")\n",
    "        \n",
    "        for i, form in enumerate(forms):\n",
    "            action = form.get('action', 'No action')\n",
    "            method = form.get('method', 'GET')\n",
    "            print(f\"   Form {i+1}: {method} -> {action}\")\n",
    "            \n",
    "            # Analyze form inputs\n",
    "            inputs = form.find_all(['input', 'select', 'button'])\n",
    "            for inp in inputs:\n",
    "                name = inp.get('name', 'No name')\n",
    "                inp_type = inp.get('type', inp.name)\n",
    "                print(f\"      - {inp_type}: {name}\")\n",
    "                \n",
    "                if inp.name == 'select':\n",
    "                    options = inp.find_all('option')\n",
    "                    print(f\"        Options: {len(options)}\")\n",
    "                    for opt in options[:3]:  # Show first 3 options\n",
    "                        value = opt.get('value', 'No value')\n",
    "                        text = opt.get_text().strip()\n",
    "                        print(f\"          {value}: {text}\")\n",
    "                    if len(options) > 3:\n",
    "                        print(f\"          ... and {len(options) - 3} more\")\n",
    "        \n",
    "        # Look for JavaScript that might handle the form\n",
    "        scripts = soup.find_all('script')\n",
    "        print(f\"\\\\nüîó Found {len(scripts)} script tags\")\n",
    "        \n",
    "        for i, script in enumerate(scripts):\n",
    "            if script.string and ('function' in script.string or 'submit' in script.string):\n",
    "                print(f\"   Script {i+1} contains functions/submit logic\")\n",
    "        \n",
    "        # Look for any direct links to Arabic pages\n",
    "        links = soup.find_all('a', href=True)\n",
    "        arabic_links = [link for link in links if 'ar/' in link.get('href', '')]\n",
    "        \n",
    "        print(f\"\\\\nüåê Found {len(arabic_links)} links to Arabic pages:\")\n",
    "        for link in arabic_links[:5]:\n",
    "            href = link.get('href')\n",
    "            text = link.get_text().strip()[:50]\n",
    "            print(f\"   {href} - {text}\")\n",
    "        \n",
    "        return soup\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error analyzing website: {e}\")\n",
    "        return None\n",
    "\n",
    "def test_direct_arabic_page():\n",
    "    \"\"\"Test accessing the Arabic version directly\"\"\"\n",
    "    print(\"\\\\nüß™ Testing direct access to Arabic pages...\")\n",
    "    \n",
    "    arabic_urls = [\n",
    "        f\"{BASE_URL}/ar/\",\n",
    "        f\"{BASE_URL}/ar/index.php\",\n",
    "        f\"{BASE_URL}/ar/dynamique/\",\n",
    "        f\"{BASE_URL}/ar/dynamique/index.php\"\n",
    "    ]\n",
    "    \n",
    "    for url in arabic_urls:\n",
    "        try:\n",
    "            print(f\"   Testing: {url}\")\n",
    "            response = session.get(url)\n",
    "            print(f\"      Status: {response.status_code}\")\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                forms = soup.find_all('form')\n",
    "                tables = soup.find_all('table')\n",
    "                selects = soup.find_all('select')\n",
    "                \n",
    "                print(f\"      Content: {len(forms)} forms, {len(tables)} tables, {len(selects)} selects\")\n",
    "                \n",
    "                # Save successful page\n",
    "                filename = url.replace('/', '_').replace(':', '').replace('.', '_') + '.html'\n",
    "                with open(f'../data/test_{filename}', 'w', encoding='utf-8') as f:\n",
    "                    f.write(soup.prettify())\n",
    "                print(f\"      Saved to: ../data/test_{filename}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"      Error: {e}\")\n",
    "\n",
    "# Run the analysis\n",
    "print(\"üîç WEBSITE STRUCTURE ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "main_soup = analyze_website_structure()\n",
    "test_direct_arabic_page()\n",
    "\n",
    "print(\"\\\\n‚úÖ Analysis complete. Check the ../data/ folder for saved HTML files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a8aa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Manual Ramz Link Discovery\n",
    "\n",
    "def discover_ramz_links_by_pattern():\n",
    "    \"\"\"Discover ramz links by testing known patterns\"\"\"\n",
    "    print(\"üîç Discovering ramz links using pattern matching...\")\n",
    "    \n",
    "    # Based on your logs, we know the pattern is:\n",
    "    # https://guide-orientation.rnu.tn/ar/dynamique/filiere.php?id=XXXXXXX\n",
    "    # Where XXXXXXX appears to be: [bac_type][ramz_code]\n",
    "    \n",
    "    discovered_links = []\n",
    "    \n",
    "    # Known bac types and their prefixes (from your previous logs)\n",
    "    bac_patterns = [\n",
    "        {'bac_value': '1', 'bac_text': 'ÿ¢ÿØÿßÿ®', 'prefix': '1'},\n",
    "        {'bac_value': '2', 'bac_text': 'ÿ±Ÿäÿßÿ∂Ÿäÿßÿ™', 'prefix': '2'}, \n",
    "        {'bac_value': '3', 'bac_text': 'ÿπŸÑŸàŸÖ ÿ™ÿ¨ÿ±Ÿäÿ®Ÿäÿ©', 'prefix': '3'},\n",
    "        {'bac_value': '4', 'bac_text': 'ÿ•ŸÇÿ™ÿµÿßÿØ Ÿàÿ™ÿµÿ±ŸÅ', 'prefix': '4'},\n",
    "        {'bac_value': '5', 'bac_text': 'ÿßŸÑÿπŸÑŸàŸÖ ÿßŸÑÿ™ŸÇŸÜŸäÿ©', 'prefix': '5'},\n",
    "        {'bac_value': '6', 'bac_text': 'ÿπŸÑŸàŸÖ ÿßŸÑÿ•ÿπŸÑÿßŸÖŸäÿ©', 'prefix': '6'},\n",
    "        {'bac_value': '7', 'bac_text': 'Other1', 'prefix': '7'},\n",
    "        {'bac_value': '8', 'bac_text': 'Other2', 'prefix': '8'},\n",
    "        {'bac_value': '9', 'bac_text': 'Other3', 'prefix': '9'}\n",
    "    ]\n",
    "    \n",
    "    # Test known ramz codes (based on your previous successful runs)\n",
    "    test_ramz_codes = [\n",
    "        '10101', '10102', '10103', '10104', '10105', '10106', '10107', '10108',\n",
    "        '10118', '10119', '10120', '10121', '10122', '10123', '10124', '10138',\n",
    "        '10139', '10160', '10162', '10167', '10190', '10191', '10192', '10193',\n",
    "        '10194', '10195', '10200', '10201', '10202', '10203'\n",
    "    ]\n",
    "    \n",
    "    print(f\"Testing {len(test_ramz_codes)} known ramz codes...\")\n",
    "    \n",
    "    for bac in bac_patterns:\n",
    "        bac_links = []\n",
    "        \n",
    "        for ramz_code in test_ramz_codes:\n",
    "            # Try different ID patterns\n",
    "            id_patterns = [\n",
    "                f\"{bac['prefix']}{ramz_code}\",  # 110101\n",
    "                f\"{ramz_code}\",                 # 10101  \n",
    "                f\"{bac['bac_value']}{ramz_code}\" # 110101\n",
    "            ]\n",
    "            \n",
    "            for ramz_id in id_patterns:\n",
    "                url = f\"{BASE_URL}/ar/dynamique/filiere.php?id={ramz_id}\"\n",
    "                \n",
    "                try:\n",
    "                    # Quick HEAD request to check if URL exists\n",
    "                    response = session.head(url, timeout=5)\n",
    "                    \n",
    "                    if response.status_code == 200:\n",
    "                        bac_links.append({\n",
    "                            'ramz_code': ramz_code,\n",
    "                            'ramz_id': ramz_id,\n",
    "                            'url': url,\n",
    "                            'bac_value': bac['bac_value'],\n",
    "                            'bac_text': bac['bac_text']\n",
    "                        })\n",
    "                        print(f\"‚úÖ Found: {ramz_code} -> {url}\")\n",
    "                        break  # Found working pattern for this ramz_code\n",
    "                        \n",
    "                except Exception:\n",
    "                    continue  # Try next pattern\n",
    "                    \n",
    "                time.sleep(0.1)  # Small delay\n",
    "        \n",
    "        if bac_links:\n",
    "            print(f\"Found {len(bac_links)} links for {bac['bac_text']}\")\n",
    "            discovered_links.extend(bac_links)\n",
    "        \n",
    "        time.sleep(1)  # Rate limiting between bac types\n",
    "    \n",
    "    print(f\"\\\\n‚úÖ Discovered {len(discovered_links)} ramz links using patterns\")\n",
    "    return discovered_links\n",
    "\n",
    "def expand_ramz_range(base_links, max_range=1000):\n",
    "    \"\"\"Expand discovered ramz links by testing nearby codes\"\"\"\n",
    "    print(f\"üîç Expanding ramz code range (testing up to {max_range} codes per bac type)...\")\n",
    "    \n",
    "    expanded_links = list(base_links)  # Start with what we have\n",
    "    \n",
    "    # Group by bac type\n",
    "    bac_groups = {}\n",
    "    for link in base_links:\n",
    "        bac_value = link['bac_value']\n",
    "        if bac_value not in bac_groups:\n",
    "            bac_groups[bac_value] = []\n",
    "        bac_groups[bac_value].append(link)\n",
    "    \n",
    "    for bac_value, links in bac_groups.items():\n",
    "        if not links:\n",
    "            continue\n",
    "            \n",
    "        bac_text = links[0]['bac_text']\n",
    "        print(f\"Expanding {bac_text}...\")\n",
    "        \n",
    "        # Get the ID pattern that worked\n",
    "        sample_link = links[0]\n",
    "        ramz_id = sample_link['ramz_id']\n",
    "        ramz_code = sample_link['ramz_code']\n",
    "        \n",
    "        # Determine the prefix pattern\n",
    "        prefix = ramz_id.replace(ramz_code, '')\n",
    "        \n",
    "        # Test a range of ramz codes\n",
    "        start_code = int(ramz_code[:3] + '01')  # e.g., 10101 -> start from 10101\n",
    "        end_code = start_code + max_range\n",
    "        \n",
    "        print(f\"   Testing range {start_code} to {end_code} with prefix '{prefix}'\")\n",
    "        \n",
    "        found_count = 0\n",
    "        for test_code in range(start_code, end_code):\n",
    "            test_ramz_code = str(test_code)\n",
    "            test_ramz_id = f\"{prefix}{test_ramz_code}\"\n",
    "            url = f\"{BASE_URL}/ar/dynamique/filiere.php?id={test_ramz_id}\"\n",
    "            \n",
    "            # Skip if we already have this one\n",
    "            if any(link['ramz_code'] == test_ramz_code for link in expanded_links):\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                response = session.head(url, timeout=3)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    expanded_links.append({\n",
    "                        'ramz_code': test_ramz_code,\n",
    "                        'ramz_id': test_ramz_id,\n",
    "                        'url': url,\n",
    "                        'bac_value': bac_value,\n",
    "                        'bac_text': bac_text\n",
    "                    })\n",
    "                    found_count += 1\n",
    "                    \n",
    "                    if found_count % 10 == 0:\n",
    "                        print(f\"      Found {found_count} new links...\")\n",
    "                        \n",
    "            except Exception:\n",
    "                continue\n",
    "                \n",
    "            time.sleep(0.05)  # Small delay\n",
    "        \n",
    "        print(f\"   Added {found_count} new links for {bac_text}\")\n",
    "        time.sleep(2)  # Rate limiting between bac types\n",
    "    \n",
    "    new_total = len(expanded_links)\n",
    "    added = new_total - len(base_links)\n",
    "    print(f\"\\\\n‚úÖ Expansion complete: {added} new links added (total: {new_total})\")\n",
    "    \n",
    "    return expanded_links\n",
    "\n",
    "# Run the alternative discovery method\n",
    "print(\"üîç ALTERNATIVE RAMZ LINK DISCOVERY\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Discover using patterns\n",
    "pattern_links = discover_ramz_links_by_pattern()\n",
    "\n",
    "if pattern_links:\n",
    "    print(f\"\\\\nüìä Pattern discovery results:\")\n",
    "    for bac_value in set(link['bac_value'] for link in pattern_links):\n",
    "        bac_links = [link for link in pattern_links if link['bac_value'] == bac_value]\n",
    "        bac_text = bac_links[0]['bac_text'] if bac_links else 'Unknown'\n",
    "        print(f\"   {bac_text}: {len(bac_links)} links\")\n",
    "    \n",
    "    # Ask user if they want to expand the range\n",
    "    print(f\"\\\\nüéØ Found {len(pattern_links)} ramz links using patterns.\")\n",
    "    print(\"You can now:\")\n",
    "    print(\"1. Use these links as-is for testing\")\n",
    "    print(\"2. Expand the range to find more links (slower)\")\n",
    "    print(\"3. Use the manual discovery as backup for the form-based method\")\n",
    "    \n",
    "    # Save the discovered links\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    pattern_file = f\"../data/pattern_discovered_ramz_{timestamp}.json\"\n",
    "    \n",
    "    with open(pattern_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(pattern_links, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"üìÅ Pattern-discovered links saved to: {pattern_file}\")\n",
    "    \n",
    "    # Store in variable for later use\n",
    "    manual_ramz_links = pattern_links\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Pattern discovery failed\")\n",
    "    manual_ramz_links = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb64e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Collect All Ramz Links (with Fallback Methods)\n",
    "\n",
    "print(\"üöÄ Starting comprehensive data collection...\")\n",
    "print(\"This cell will try multiple methods to collect ramz links\")\n",
    "\n",
    "# Method 1: Try form-based extraction\n",
    "print(\"\\nüìã Method 1: Form-based extraction\")\n",
    "bac_types = get_bac_types()\n",
    "\n",
    "if not bac_types:\n",
    "    print(\"‚ùå Failed to get baccalaureate types via forms\")\n",
    "    bac_types = []\n",
    "else:\n",
    "    print(f\"‚úÖ Found {len(bac_types)} baccalaureate types:\")\n",
    "    for i, bac in enumerate(bac_types, 1):\n",
    "        print(f\"   {i}. {bac['text']} (value: {bac['value']})\")\n",
    "\n",
    "# Collect ramz links for all bac types\n",
    "all_ramz_links = []\n",
    "\n",
    "if bac_types:\n",
    "    print(\"\\nüîó Collecting ramz links via form submission...\")\n",
    "    \n",
    "    for bac in tqdm(bac_types, desc=\"Processing bac types\"):\n",
    "        ramz_links = get_ramz_links_for_bac(bac['value'], bac['text'])\n",
    "        if ramz_links:\n",
    "            all_ramz_links.extend(ramz_links)\n",
    "            print(f\"   ‚úÖ {bac['text']}: {len(ramz_links)} links\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå {bac['text']}: No links found\")\n",
    "        time.sleep(2)  # Rate limiting\n",
    "\n",
    "print(f\"\\nüìä Form-based method results: {len(all_ramz_links)} ramz links\")\n",
    "\n",
    "# Method 2: Use manual discovery if form method failed or found few links\n",
    "if len(all_ramz_links) < 100:  # Threshold for \"successful\" extraction\n",
    "    print(\"\\nüîß Method 2: Pattern-based discovery (fallback)\")\n",
    "    print(\"Form-based method didn't find enough links. Using alternative method...\")\n",
    "    \n",
    "    if 'manual_ramz_links' in locals() and manual_ramz_links:\n",
    "        print(f\"‚úÖ Using previously discovered {len(manual_ramz_links)} links from pattern method\")\n",
    "        all_ramz_links = manual_ramz_links\n",
    "    else:\n",
    "        print(\"Running pattern discovery now...\")\n",
    "        manual_links = discover_ramz_links_by_pattern()\n",
    "        if manual_links:\n",
    "            all_ramz_links = manual_links\n",
    "            print(f\"‚úÖ Pattern method found {len(manual_links)} links\")\n",
    "        else:\n",
    "            print(\"‚ùå Pattern method also failed\")\n",
    "\n",
    "# Method 3: Load from existing file if available\n",
    "if len(all_ramz_links) == 0:\n",
    "    print(\"\\nüìÅ Method 3: Load from existing file\")\n",
    "    \n",
    "    # Look for existing ramz link files\n",
    "    import glob\n",
    "    existing_files = glob.glob('../data/*ramz*.json')\n",
    "    \n",
    "    if existing_files:\n",
    "        latest_file = max(existing_files, key=os.path.getctime)\n",
    "        print(f\"Found existing file: {latest_file}\")\n",
    "        \n",
    "        try:\n",
    "            with open(latest_file, 'r', encoding='utf-8') as f:\n",
    "                file_links = json.load(f)\n",
    "            \n",
    "            if isinstance(file_links, list) and len(file_links) > 0:\n",
    "                all_ramz_links = file_links\n",
    "                print(f\"‚úÖ Loaded {len(file_links)} links from {latest_file}\")\n",
    "            else:\n",
    "                print(\"‚ùå File doesn't contain valid ramz links\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading file: {e}\")\n",
    "    else:\n",
    "        print(\"‚ùå No existing ramz link files found\")\n",
    "\n",
    "# Final results\n",
    "print(f\"\\nüéØ FINAL RESULTS\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "if all_ramz_links:\n",
    "    print(f\"‚úÖ Total ramz links collected: {len(all_ramz_links)}\")\n",
    "    \n",
    "    # Analyze the collected links\n",
    "    bac_distribution = {}\n",
    "    for link in all_ramz_links:\n",
    "        bac_text = link.get('bac_text', 'Unknown')\n",
    "        bac_distribution[bac_text] = bac_distribution.get(bac_text, 0) + 1\n",
    "    \n",
    "    print(f\"\\nüìä Distribution by baccalaureate type:\")\n",
    "    for bac_text, count in sorted(bac_distribution.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"   {bac_text}: {count} specializations\")\n",
    "    \n",
    "    # Save ramz links\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    ramz_links_file = f\"../data/collected_ramz_links_{timestamp}.json\"\n",
    "    \n",
    "    with open(ramz_links_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_ramz_links, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\nüìÅ Ramz links saved to: {ramz_links_file}\")\n",
    "    \n",
    "    # Show sample ramz links\n",
    "    print(f\"\\nüìã Sample ramz links:\")\n",
    "    for i, link in enumerate(all_ramz_links[:5], 1):\n",
    "        print(f\"   {i}. {link['ramz_code']} ({link.get('bac_text', 'Unknown')}) -> {link['url']}\")\n",
    "    \n",
    "    if len(all_ramz_links) > 5:\n",
    "        print(f\"   ... and {len(all_ramz_links) - 5} more\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Ready to proceed with scraping {len(all_ramz_links)} specializations!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Failed to collect any ramz links using all methods\")\n",
    "    print(\"\\nTroubleshooting suggestions:\")\n",
    "    print(\"1. Check your internet connection\")\n",
    "    print(\"2. Verify the website is accessible\")\n",
    "    print(\"3. Run the website structure analysis cell\")\n",
    "    print(\"4. Check if the website structure has changed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874eae76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Scrape All Ramz Details in Parallel\n",
    "\n",
    "async def scrape_all_ramz_parallel(ramz_links, max_concurrent=20):\n",
    "    \"\"\"Scrape all ramz details using async parallel processing\"\"\"\n",
    "    print(f\"‚ö° Starting parallel scraping of {len(ramz_links)} ramz pages...\")\n",
    "    \n",
    "    # Create semaphore to limit concurrent requests\n",
    "    semaphore = asyncio.Semaphore(max_concurrent)\n",
    "    \n",
    "    async def fetch_with_semaphore(session, ramz_info):\n",
    "        async with semaphore:\n",
    "            await asyncio.sleep(DELAY)  # Rate limiting\n",
    "            return await fetch_html_content(session, ramz_info)\n",
    "    \n",
    "    # Create aiohttp session\n",
    "    timeout = aiohttp.ClientTimeout(total=30)\n",
    "    async with aiohttp.ClientSession(timeout=timeout) as async_session:\n",
    "        # Create tasks for all ramz links\n",
    "        tasks = [\n",
    "            fetch_with_semaphore(async_session, ramz_info) \n",
    "            for ramz_info in ramz_links\n",
    "        ]\n",
    "        \n",
    "        # Execute all tasks with progress bar\n",
    "        results = []\n",
    "        failed = []\n",
    "        \n",
    "        # Process tasks in batches to avoid overwhelming\n",
    "        batch_size = 100\n",
    "        for i in range(0, len(tasks), batch_size):\n",
    "            batch = tasks[i:i + batch_size]\n",
    "            batch_results = await asyncio.gather(*batch, return_exceptions=True)\n",
    "            \n",
    "            for result in batch_results:\n",
    "                if isinstance(result, Exception):\n",
    "                    failed.append(str(result))\n",
    "                elif result[1] is not None:  # HTML content received\n",
    "                    results.append(result)\n",
    "                else:\n",
    "                    failed.append(result[0]['ramz_code'])\n",
    "            \n",
    "            print(f\"Progress: {min(i + batch_size, len(tasks))}/{len(tasks)} \"\n",
    "                  f\"({len(results)} successful, {len(failed)} failed)\")\n",
    "    \n",
    "    print(f\"\\\\n‚úÖ Scraping completed: {len(results)} successful, {len(failed)} failed\")\n",
    "    return results, failed\n",
    "\n",
    "# Only proceed if we have ramz links\n",
    "if 'all_ramz_links' in locals() and all_ramz_links:\n",
    "    # For testing, limit to first 50 ramz links\n",
    "    test_links = all_ramz_links[:50]  # Remove this line for full scraping\n",
    "    \n",
    "    print(f\"üß™ Testing with {len(test_links)} ramz links (modify to scrape all {len(all_ramz_links)})...\")\n",
    "    \n",
    "    # Run the async scraping\n",
    "    scraped_results, failed_ramz = await scrape_all_ramz_parallel(test_links, max_concurrent=15)\n",
    "    \n",
    "    print(f\"\\\\nüìä Results summary:\")\n",
    "    print(f\"   - Successfully scraped: {len(scraped_results)}\")\n",
    "    print(f\"   - Failed: {len(failed_ramz)}\")\n",
    "    \n",
    "    # Save raw HTML results\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    html_results_file = f\"../data/raw_html_results_{timestamp}.json\"\n",
    "    \n",
    "    # Prepare data for saving (ramz_info + HTML)\n",
    "    html_data = []\n",
    "    for ramz_info, html_content in scraped_results:\n",
    "        html_data.append({\n",
    "            'ramz_info': ramz_info,\n",
    "            'html_content': html_content,\n",
    "            'scraped_at': datetime.now().isoformat()\n",
    "        })\n",
    "    \n",
    "    with open(html_results_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(html_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"üìÅ Raw HTML results saved to: {html_results_file}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No ramz links available. Run the previous cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d0aa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Parse All Scraped HTML Data\n",
    "\n",
    "def parse_all_scraped_data(scraped_results):\n",
    "    \"\"\"Parse all scraped HTML data into structured format\"\"\"\n",
    "    print(f\"üîç Parsing {len(scraped_results)} scraped HTML pages...\")\n",
    "    \n",
    "    parsed_data = []\n",
    "    parsing_errors = []\n",
    "    \n",
    "    for ramz_info, html_content in tqdm(scraped_results, desc=\"Parsing HTML\"):\n",
    "        if html_content:\n",
    "            try:\n",
    "                detail = parse_ramz_detail_page(ramz_info, html_content)\n",
    "                if detail:\n",
    "                    parsed_data.append(detail)\n",
    "                else:\n",
    "                    parsing_errors.append(ramz_info['ramz_code'])\n",
    "            except Exception as e:\n",
    "                parsing_errors.append(f\"{ramz_info['ramz_code']}: {str(e)}\")\n",
    "        else:\n",
    "            parsing_errors.append(f\"{ramz_info['ramz_code']}: No HTML content\")\n",
    "    \n",
    "    print(f\"\\\\n‚úÖ Parsing completed:\")\n",
    "    print(f\"   - Successfully parsed: {len(parsed_data)}\")\n",
    "    print(f\"   - Parsing errors: {len(parsing_errors)}\")\n",
    "    \n",
    "    return parsed_data, parsing_errors\n",
    "\n",
    "# Parse the scraped data\n",
    "if 'scraped_results' in locals() and scraped_results:\n",
    "    parsed_specializations, parse_errors = parse_all_scraped_data(scraped_results)\n",
    "    \n",
    "    if parsed_specializations:\n",
    "        print(f\"\\\\nüìä Sample parsed data for ramz {parsed_specializations[0].ramz_code}:\")\n",
    "        sample = parsed_specializations[0]\n",
    "        \n",
    "        sample_fields = [\n",
    "            ('University', sample.university),\n",
    "            ('Governorate', sample.governorate), \n",
    "            ('Institution', sample.institution),\n",
    "            ('Address', sample.address),\n",
    "            ('Phone', sample.phone),\n",
    "            ('Specialization', sample.specialization),\n",
    "            ('Training Field', sample.training_field),\n",
    "            ('Measure', sample.measure),\n",
    "            ('Bac Type', sample.bac_type),\n",
    "            ('Capacity 2025', sample.capacity_2025),\n",
    "            ('Requires Test', sample.requires_test),\n",
    "            ('Geographic Distribution', sample.geographic_distribution),\n",
    "            ('Conditions', sample.conditions),\n",
    "            ('Study Duration', sample.study_duration),\n",
    "            ('Last Score 2024', sample.last_oriented_score_2024),\n",
    "            ('Score History', str(sample.score_history))\n",
    "        ]\n",
    "        \n",
    "        for field, value in sample_fields:\n",
    "            display_value = value[:50] + \"...\" if len(str(value)) > 50 else value\n",
    "            print(f\"   {field}: {display_value}\")\n",
    "        \n",
    "        # Calculate field completeness\n",
    "        print(f\"\\\\nüìà Data completeness analysis:\")\n",
    "        field_counts = {}\n",
    "        total_records = len(parsed_specializations)\n",
    "        \n",
    "        for spec in parsed_specializations:\n",
    "            for field, value in asdict(spec).items():\n",
    "                if value and str(value).strip():\n",
    "                    field_counts[field] = field_counts.get(field, 0) + 1\n",
    "        \n",
    "        # Sort by completeness\n",
    "        sorted_fields = sorted(field_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        for field, count in sorted_fields:\n",
    "            percentage = (count / total_records) * 100\n",
    "            print(f\"   {field}: {count}/{total_records} ({percentage:.1f}%)\")\n",
    "    \n",
    "    else:\n",
    "        print(\"‚ùå No data was successfully parsed\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No scraped results available. Run the scraping cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad132d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Save Parsed Data to CSV and JSON\n",
    "\n",
    "def save_results_to_files(parsed_data, filename_prefix=\"tunisia_university_data\"):\n",
    "    \"\"\"Save parsed data to both CSV and JSON formats\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Convert to list of dictionaries\n",
    "    data_dicts = [asdict(spec) for spec in parsed_data]\n",
    "    \n",
    "    # Save as JSON\n",
    "    json_filename = f\"../data/{filename_prefix}_{timestamp}.json\"\n",
    "    with open(json_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data_dicts, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # Save as CSV\n",
    "    csv_filename = f\"../data/{filename_prefix}_{timestamp}.csv\"\n",
    "    if data_dicts:\n",
    "        df = pd.DataFrame(data_dicts)\n",
    "        \n",
    "        # Convert score_history dict to JSON string for CSV\n",
    "        df['score_history'] = df['score_history'].apply(lambda x: json.dumps(x, ensure_ascii=False))\n",
    "        \n",
    "        # Save to CSV\n",
    "        df.to_csv(csv_filename, index=False, encoding='utf-8')\n",
    "        \n",
    "        # Display DataFrame info\n",
    "        print(f\"üìä DataFrame shape: {df.shape}\")\n",
    "        print(f\"üìÅ Files saved:\")\n",
    "        print(f\"   - JSON: {json_filename}\")\n",
    "        print(f\"   - CSV: {csv_filename}\")\n",
    "        \n",
    "        return df, json_filename, csv_filename\n",
    "    \n",
    "    return None, json_filename, csv_filename\n",
    "\n",
    "# Save the results\n",
    "if 'parsed_specializations' in locals() and parsed_specializations:\n",
    "    print(f\"üíæ Saving {len(parsed_specializations)} parsed specializations...\")\n",
    "    \n",
    "    df_results, json_file, csv_file = save_results_to_files(parsed_specializations)\n",
    "    \n",
    "    if df_results is not None:\n",
    "        print(f\"\\\\n‚úÖ Data successfully saved!\")\n",
    "        \n",
    "        # Display basic statistics\n",
    "        print(f\"\\\\nüìä Dataset Overview:\")\n",
    "        print(f\"   - Total records: {len(df_results)}\")\n",
    "        print(f\"   - Total columns: {len(df_results.columns)}\")\n",
    "        print(f\"   - Memory usage: {df_results.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "        \n",
    "        # Show column data types\n",
    "        print(f\"\\\\nüìã Column Information:\")\n",
    "        for col in df_results.columns:\n",
    "            non_null_count = df_results[col].notna().sum()\n",
    "            data_type = df_results[col].dtype\n",
    "            print(f\"   {col}: {non_null_count}/{len(df_results)} non-null, {data_type}\")\n",
    "        \n",
    "        # Show universities distribution\n",
    "        if 'university' in df_results.columns:\n",
    "            university_counts = df_results['university'].value_counts().head(10)\n",
    "            print(f\"\\\\nüèõÔ∏è Top 10 Universities by number of specializations:\")\n",
    "            for univ, count in university_counts.items():\n",
    "                if univ and univ.strip():\n",
    "                    print(f\"   {univ}: {count} specializations\")\n",
    "        \n",
    "        # Show bac types distribution\n",
    "        if 'bac_type' in df_results.columns:\n",
    "            bac_counts = df_results['bac_type'].value_counts()\n",
    "            print(f\"\\\\nüéì Baccalaureate Types Distribution:\")\n",
    "            for bac, count in bac_counts.items():\n",
    "                if bac and bac.strip():\n",
    "                    print(f\"   {bac}: {count} specializations\")\n",
    "    \n",
    "    else:\n",
    "        print(\"‚ùå Failed to save data\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No parsed data available. Run the parsing cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785f7528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Data Analysis and Visualization\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "def analyze_scraped_data(df):\n",
    "    \"\"\"Perform comprehensive analysis of the scraped data\"\"\"\n",
    "    \n",
    "    if df is None or df.empty:\n",
    "        print(\"‚ùå No data available for analysis\")\n",
    "        return\n",
    "    \n",
    "    print(\"üìä COMPREHENSIVE DATA ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. Basic Statistics\n",
    "    print(f\"\\\\nüìà Basic Statistics:\")\n",
    "    print(f\"   Total Specializations: {len(df)}\")\n",
    "    print(f\"   Unique Universities: {df['university'].nunique()}\")\n",
    "    print(f\"   Unique Institutions: {df['institution'].nunique()}\")\n",
    "    print(f\"   Unique Bac Types: {df['bac_type'].nunique()}\")\n",
    "    \n",
    "    # 2. Data Completeness Heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Calculate completeness for each field\n",
    "    completeness = {}\n",
    "    for col in df.columns:\n",
    "        if col not in ['score_history', 'extraction_timestamp']:\n",
    "            non_empty = df[col].notna() & (df[col] != '') & (df[col] != '-')\n",
    "            completeness[col] = non_empty.sum() / len(df) * 100\n",
    "    \n",
    "    # Create completeness visualization\n",
    "    plt.subplot(2, 2, 1)\n",
    "    fields = list(completeness.keys())\n",
    "    values = list(completeness.values())\n",
    "    \n",
    "    plt.barh(fields, values, color=sns.color_palette(\"viridis\", len(fields)))\n",
    "    plt.xlabel('Completeness (%)')\n",
    "    plt.title('Data Completeness by Field')\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # 3. University Distribution\n",
    "    plt.subplot(2, 2, 2)\n",
    "    top_universities = df['university'].value_counts().head(10)\n",
    "    if not top_universities.empty:\n",
    "        plt.pie(top_universities.values, labels=top_universities.index, autopct='%1.1f%%')\n",
    "        plt.title('Top 10 Universities Distribution')\n",
    "    \n",
    "    # 4. Bac Type Distribution\n",
    "    plt.subplot(2, 2, 3)\n",
    "    bac_counts = df['bac_type'].value_counts()\n",
    "    if not bac_counts.empty:\n",
    "        plt.bar(range(len(bac_counts)), bac_counts.values)\n",
    "        plt.xticks(range(len(bac_counts)), bac_counts.index, rotation=45, ha='right')\n",
    "        plt.title('Specializations by Baccalaureate Type')\n",
    "        plt.ylabel('Count')\n",
    "    \n",
    "    # 5. Score Analysis (if available)\n",
    "    plt.subplot(2, 2, 4)\n",
    "    if 'last_oriented_score_2024' in df.columns:\n",
    "        # Convert scores to numeric\n",
    "        scores = pd.to_numeric(df['last_oriented_score_2024'], errors='coerce')\n",
    "        scores = scores.dropna()\n",
    "        \n",
    "        if not scores.empty:\n",
    "            plt.hist(scores, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "            plt.xlabel('Last Oriented Score 2024')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.title('Distribution of Admission Scores 2024')\n",
    "            plt.grid(alpha=0.3)\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, 'No score data available', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 6. Detailed Statistics\n",
    "    print(f\"\\\\nüìã Detailed Field Analysis:\")\n",
    "    for field, completeness_pct in sorted(completeness.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"   {field}: {completeness_pct:.1f}% complete\")\n",
    "    \n",
    "    # 7. Sample Records\n",
    "    print(f\"\\\\nüìÑ Sample Records (first 3):\")\n",
    "    for i, (idx, row) in enumerate(df.head(3).iterrows()):\n",
    "        print(f\"\\\\n   Record {i+1} (Ramz: {row.get('ramz_code', 'N/A')}):\")\n",
    "        for field in ['university', 'institution', 'specialization', 'bac_type', 'last_oriented_score_2024']:\n",
    "            value = row.get(field, 'N/A')\n",
    "            display_value = value[:60] + \"...\" if len(str(value)) > 60 else value\n",
    "            print(f\"      {field}: {display_value}\")\n",
    "\n",
    "# Run the analysis\n",
    "if 'df_results' in locals() and df_results is not None:\n",
    "    analyze_scraped_data(df_results)\n",
    "    \n",
    "    print(f\"\\\\nüéâ SCRAPING PROJECT COMPLETED!\")\n",
    "    print(f\"=\" * 50)\n",
    "    print(f\"‚úÖ Successfully scraped and analyzed Tunisian university data\")\n",
    "    print(f\"üìä Total records: {len(df_results)}\")\n",
    "    print(f\"üìÅ Data saved to CSV and JSON formats\")\n",
    "    print(f\"üîç Analysis and visualizations generated\")\n",
    "    \n",
    "    print(f\"\\\\nüìù Next Steps:\")\n",
    "    print(f\"   1. Review the generated CSV file for data quality\")\n",
    "    print(f\"   2. Perform additional analysis as needed\")\n",
    "    print(f\"   3. Use the data for your research or application\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No data available for analysis. Please run all previous cells first.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
