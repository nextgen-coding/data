# Tunisian University Orientation Guide Scraper

A comprehensive web scraping tool for extracting data from the Tunisian University Orientation website (https://guide-orientation.rnu.tn/).

## ğŸ¯ Project Overview

This project scrapes detailed information about Tunisian universities, institutions, and academic programs including:
- University and institution details
- Program specifications and requirements
- Historical admission scores
- Baccalaureate requirements
- Geographic distribution information

## ğŸ“Š Data Structure

The scraper extracts the following information for each specialization:

| Field | Description | Example |
|-------|-------------|---------|
| `ramz_code` | Specialization code | "10101" |
| `university` | University name | "Ø¬Ø§Ù…Ø¹Ø© ØªÙˆÙ†Ø³" |
| `governorate` | Governorate | "ØªÙˆÙ†Ø³" |
| `institution` | Institution name | "ÙƒÙ„ÙŠØ© Ø§Ù„Ø¹Ù„ÙˆÙ… Ø§Ù„Ø¥Ù†Ø³Ø§Ù†ÙŠØ© ÙˆØ§Ù„Ø¥Ø¬ØªÙ…Ø§Ø¹ÙŠØ© Ø¨ØªÙˆÙ†Ø³" |
| `address` | Institution address | "94 Ø´Ø§Ø±Ø¹ 9 Ø£ÙØ±ÙŠÙ„ 1938Ù€ ØªÙ€ÙˆÙ†Ø³ 1007" |
| `phone` | Contact phone | "71564713/71564797" |
| `specialization` | Program name | "Ø§Ù„Ø¥Ø¬Ø§Ø²Ø© ÙÙŠ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©" |
| `training_field` | Training domain | "Ø§Ù„Ø¢Ø¯Ø§Ø¨ ÙˆØ§Ù„Ù„ØºØ§Øª ÙˆØ§Ù„Ù…Ø±Ø§Ø­Ù„ Ø§Ù„ØªØ­Ø¶ÙŠØ±ÙŠØ© Ø§Ù„Ø£Ø¯Ø¨ÙŠØ©" |
| `specializations` | Specializations | "Ø§Ù„Ù„ØºØ© ÙˆØ§Ù„Ø¢Ø¯Ø§Ø¨ ÙˆØ§Ù„Ø­Ø¶Ø§Ø±Ø©" |
| `measure` | Measurement criteria | "FG+AR" |
| `bac_type` | Baccalaureate type | "Ø¢Ø¯Ø§Ø¨" |
| `capacity_2025` | 2025 capacity | "50" |
| `requires_test` | Requires entrance test | "Ù„Ø§" |
| `geographic_distribution` | Geographic distribution (7%) | "Ù†Ø¹Ù…" |
| `conditions` | Entry conditions | "Ù„Ø§Ø´Ø¦" |
| `study_duration` | Study duration | "03 Ø³Ù†ÙˆØ§Øª" |
| `last_oriented_score_2024` | Last admission score 2024 | "97.8750" |
| `score_history` | Historical scores by year | `{"2023": "95.2", "2024": "97.8"}` |

## ğŸš€ Getting Started

### Prerequisites

- Python 3.8+
- pip

### Installation

1. Clone or download this project
2. Install dependencies:

```bash
pip install -r requirements.txt
```

### Dependencies

- `requests` - HTTP library for making web requests
- `beautifulsoup4` - HTML parsing library
- `selenium` - Web browser automation (for complex interactions)
- `pandas` - Data manipulation and analysis
- `aiohttp` - Asynchronous HTTP client
- `fake-useragent` - Generate fake user agent headers
- `tqdm` - Progress bars
- `matplotlib`, `seaborn` - Data visualization
- `jupyter` - Interactive notebooks

## ğŸ“ Project Structure

```
ba/
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ README.md                # This file
â”œâ”€â”€ website_analyzer.py      # Website structure analysis
â”œâ”€â”€ university_scraper.py    # Main comprehensive scraper
â”œâ”€â”€ test_single_ramz.py     # Test script for single page
â”œâ”€â”€ data/                   # Directory for scraped data
â”œâ”€â”€ scrapers/              # Specialized scrapers
â”œâ”€â”€ analysis/              # Data analysis tools
â”œâ”€â”€ notebooks/             # Jupyter notebooks
â””â”€â”€ .github/
    â””â”€â”€ copilot-instructions.md
```

## ğŸ”§ Usage

### 1. Test Single Page Scraping

First, test the scraper on a single page to understand the data structure:

```bash
python test_single_ramz.py
```

This will:
- Test scraping a few sample ramz pages
- Show you the exact data structure
- Create a sample CSV file
- Display what information can be extracted

### 2. Analyze Website Structure

Understand the website's dropdown options and form structure:

```bash
python website_analyzer.py
```

### 3. Full Comprehensive Scraping

Run the complete scraper to extract all data:

```bash
python university_scraper.py
```

This will:
1. **Analyze dropdown options** - Extract all baccalaureate types
2. **Collect ramz links** - Get all specialization links for each bac type
3. **Test scraping** - Test on a few samples first
4. **Full parallel scraping** - Scrape all data using multiple threads
5. **Save results** - Export to both JSON and CSV formats

## âš¡ Parallel Processing

The scraper uses parallel processing to speed up data collection:

- **Default**: 5 concurrent workers
- **Adjustable**: Modify `max_workers` parameter
- **Rate limiting**: Built-in delays to respect server limits
- **Error handling**: Robust error handling and retry mechanisms

## ğŸ“Š Output Formats

### CSV Output
- Clean tabular format
- Easy to import into Excel/Google Sheets
- Score history as JSON string

### JSON Output
- Preserves complex data structures
- Machine-readable format
- Includes raw HTML for debugging

## ğŸ” Workflow Explanation

The scraping process follows this workflow:

1. **Main Page Analysis**
   ```
   https://guide-orientation.rnu.tn/ â†’ Extract dropdown options
   ```

2. **Search Form Submission**
   ```
   Select bac type â†’ Submit form â†’ Get search results
   ```

3. **Ramz Link Extraction**
   ```
   Search results table â†’ Extract ramz popup links
   ```

4. **Detailed Page Scraping**
   ```
   ramz popup pages â†’ Extract detailed information
   ```

## ğŸ› ï¸ Technical Details

### URL Pattern
- Main search: `https://guide-orientation.rnu.tn/ar/dynamique/index_ar.php`
- Detail pages: `https://guide-orientation.rnu.tn/ar/dynamique/filiere.php?id=XXXXXXX`

### JavaScript Handling
The website uses JavaScript popups for detail pages:
```javascript
PopupCentrer("ar/dynamique/filiere.php?id=410104", 800, 750, "toolbar=no...")
```

### Arabic Text Extraction
Uses regex patterns to extract Arabic labels:
- `Ø§Ù„Ø¬Ø§Ù…Ø¹Ø©` (University)
- `Ø§Ù„Ù…Ø¤Ø³Ø³Ø©` (Institution)  
- `Ø§Ù„Ø´Ø¹Ø¨Ø© / Ø§Ù„Ø¥Ø¬Ø§Ø²Ø©` (Specialization)
- etc.

## ğŸ“ˆ Expected Results

Based on your previous runs:
- **~3,161 total ramz links** across all bac types
- **Multiple baccalaureate types** (Ø¢Ø¯Ø§Ø¨, Ø¹Ù„ÙˆÙ…, ØªÙ‚Ù†ÙŠ, etc.)
- **Comprehensive coverage** of all Tunisian universities

## ğŸš¨ Important Notes

### Ethical Scraping
- Respects robots.txt
- Implements rate limiting
- Uses appropriate user agents
- Includes error handling

### Rate Limiting
- 1-2 second delays between requests
- Adjustable based on server response
- Parallel workers with distributed delays

### Error Handling
- Robust exception handling
- Detailed logging
- Failed request tracking
- Retry mechanisms

## ğŸ›ï¸ Configuration

You can customize the scraper behavior:

```python
scraper = UniversityGuideScraper(
    max_workers=8,    # Number of parallel workers
    delay=1           # Delay between requests (seconds)
)
```

## ğŸ“‹ Sample Output

A complete record looks like this:

```csv
ramz_code,university,governorate,institution,specialization,last_oriented_score_2024,...
10101,Ø¬Ø§Ù…Ø¹Ø© ØªÙˆÙ†Ø³,ØªÙˆÙ†Ø³,ÙƒÙ„ÙŠØ© Ø§Ù„Ø¹Ù„ÙˆÙ… Ø§Ù„Ø¥Ù†Ø³Ø§Ù†ÙŠØ© ÙˆØ§Ù„Ø¥Ø¬ØªÙ…Ø§Ø¹ÙŠØ© Ø¨ØªÙˆÙ†Ø³,Ø§Ù„Ø¥Ø¬Ø§Ø²Ø© ÙÙŠ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©,97.8750,...
```

## ğŸ”„ Running the Complete Scraper

1. **Start with test**: `python test_single_ramz.py`
2. **Verify structure**: Check the generated CSV sample
3. **Run full scraper**: `python university_scraper.py`
4. **Monitor progress**: Watch the console output
5. **Check results**: Look in the `data/` folder

The scraper will save timestamped files:
- `complete_university_data_YYYYMMDD_HHMMSS.json`
- `complete_university_data_YYYYMMDD_HHMMSS.csv`

## ğŸ¤ Contributing

Feel free to improve the scraper by:
- Adding more data fields
- Improving Arabic text extraction
- Optimizing performance
- Adding data analysis features

## âš ï¸ Disclaimer

This tool is for educational and research purposes. Always respect the website's terms of service and rate limits.
